{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA before logistic regression for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing LDA as pre-processing step before logistic regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing the dataset\n",
    "data = pd.read_csv('Heart.csv')\n",
    "# convert to numpy array\n",
    "data = data.values\n",
    "# remove the first column\n",
    "data = data[:,1:]\n",
    "for i in range(data.shape[0]):\n",
    "    if data[i, 2] == 'asymptomatic':\n",
    "        data[i, 2] = 0\n",
    "    elif data[i, 2] == 'nonanginal':\n",
    "        data[i, 2] = 1\n",
    "    elif data[i, 2] == 'nontypical':\n",
    "        data[i, 2] = 2\n",
    "    elif data[i, 2] == 'typical':\n",
    "        data[i, 2] = 3\n",
    "    if data[i,13] == 'Yes':\n",
    "        data[i,13] = 1\n",
    "    elif data[i,13] == 'No':\n",
    "        data[i,13] = 0\n",
    "    if data[i,12] == 'fixed':\n",
    "        data[i,12] = 0\n",
    "    elif data[i,12] == 'reversable':\n",
    "        data[i,12] = 1\n",
    "    if data[i,12] == 'normal':\n",
    "        data[i,12] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to float\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[:,0:13], data[:,13], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform fisher linear discriminant on the data\n",
    "# separate the data into two classes\n",
    "mean_classes = np.zeros((2,13))\n",
    "mean_classes[0,:] = np.mean(X_train[Y_train==0,:], axis=0)\n",
    "mean_classes[1,:] = np.mean(X_train[Y_train==1,:], axis=0)\n",
    "# compute the overall mean\n",
    "mean_overall = np.mean(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the within class scatter matrix\n",
    "S_W = np.zeros((13,13))\n",
    "for i in range(2):\n",
    "    S_i = np.zeros((13,13))\n",
    "    for j in range(X_train[Y_train==i,:].shape[0]):\n",
    "        x = X_train[Y_train==i,:][j,:].reshape(13,1)\n",
    "        mean = mean_classes[i,:].reshape(13,1)\n",
    "        S_i += (x-mean).dot((x-mean).T)\n",
    "    S_W += S_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the between class scatter matrix\n",
    "S_B = np.zeros((13,13))\n",
    "for i in range(2):\n",
    "    n_i = X_train[Y_train==i,:].shape[0]\n",
    "    mean_i = mean_classes[i,:].reshape(13,1)\n",
    "    mean_overall = mean_overall.reshape(13,1)\n",
    "    S_B += n_i*(mean_i-mean_overall).dot((mean_i-mean_overall).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the eigenvalues and eigenvectors of inv(S_W).dot(S_B)\n",
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "# sort the eigenvalues in descending order\n",
    "indices = np.argsort(eig_vals)[::-1]\n",
    "eig_vals = eig_vals[indices]\n",
    "eig_vecs = eig_vecs[:,indices]\n",
    "# select the eigenvectors corresponding to the largest eigenvalues\n",
    "eigen_vector = eig_vecs[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the data onto the new subspace\n",
    "X_train_fda = X_train.dot(eigen_vector)\n",
    "X_test_fda = X_test.dot(eigen_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the logistic regression function\n",
    "learning_rate = 0.01\n",
    "no_of_iterations = 2000\n",
    "# add a column of ones to the data\n",
    "X_train_fda = np.hstack((np.ones((X_train_fda.shape[0],1)), X_train_fda))\n",
    "# define a vector of weights of size dimension of data\n",
    "weights = np.zeros((X_train_fda.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply the weights with the data\n",
    "def combine(X, weights):\n",
    "    return X.dot(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the gradient descent algorithm to find the required weights\n",
    "prev_weights = np.zeros((X_train_fda.shape[1],1))\n",
    "for i in range(no_of_iterations):\n",
    "    z_value = combine(X_train_fda, weights)\n",
    "    Y_predicted = sigmoid(z_value)\n",
    "    # round the values to 0 or 1\n",
    "    Y_predicted[Y_predicted>=0.5] = 1\n",
    "    Y_predicted[Y_predicted<0.5] = 0\n",
    "    # compute the error\n",
    "    error = Y_predicted - Y_train.reshape(Y_train.shape[0],1)\n",
    "    # compute the gradient\n",
    "    gradient = X_train_fda.T.dot(error)\n",
    "    # assign the current weights to the previous weights\n",
    "    prev_weights = weights\n",
    "    # update the weights\n",
    "    weights = weights - learning_rate*gradient/X_train_fda.shape[0]\n",
    "    # check if the weights have converged \n",
    "    if np.linalg.norm(weights-prev_weights) < 1e-4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  85.0 %\n"
     ]
    }
   ],
   "source": [
    "# compute the test accuracy\n",
    "X_test_fda = np.hstack((np.ones((X_test_fda.shape[0],1)), X_test_fda))\n",
    "z_value = combine(X_test_fda, weights)\n",
    "Y_predicted = sigmoid(z_value)\n",
    "Y_predicted[Y_predicted>=0.5] = 1\n",
    "Y_predicted[Y_predicted<0.5] = 0\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for i in range(Y_test.shape[0]):\n",
    "    if Y_test[i] == 0 and Y_predicted[i] == 0:\n",
    "        count_0 += 1\n",
    "    elif Y_test[i] == 1 and Y_predicted[i] == 1:\n",
    "        count_1 += 1\n",
    "print('Test accuracy: ', (count_0+count_1)/Y_test.shape[0]*100 ,'%')\n",
    "test_accuracy_lda = (count_0+count_1)/Y_test.shape[0]*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + LDA as pre-processing steps before logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing PCA before LDA and logistic regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing the dataset\n",
    "data = pd.read_csv('Heart.csv')\n",
    "# convert to numpy array\n",
    "data = data.values\n",
    "# remove the first column\n",
    "data = data[:,1:]\n",
    "for i in range(data.shape[0]):\n",
    "    if data[i, 2] == 'asymptomatic':\n",
    "        data[i, 2] = 0\n",
    "    elif data[i, 2] == 'nonanginal':\n",
    "        data[i, 2] = 1\n",
    "    elif data[i, 2] == 'nontypical':\n",
    "        data[i, 2] = 2\n",
    "    elif data[i, 2] == 'typical':\n",
    "        data[i, 2] = 3\n",
    "    if data[i,13] == 'Yes':\n",
    "        data[i,13] = 1\n",
    "    elif data[i,13] == 'No':\n",
    "        data[i,13] = 0\n",
    "    if data[i,12] == 'fixed':\n",
    "        data[i,12] = 0\n",
    "    elif data[i,12] == 'reversable':\n",
    "        data[i,12] = 1\n",
    "    if data[i,12] == 'normal':\n",
    "        data[i,12] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to float\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[:,0:13], data[:,13], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "no_of_PCs = 9\n",
    "pca = PCA(n_components=no_of_PCs)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# transform the test data using the same PCA object\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train = X_train_pca\n",
    "X_test = X_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform fisher linear discriminant on the data\n",
    "# separate the data into two classes\n",
    "mean_classes = np.zeros((2,no_of_PCs))\n",
    "mean_classes[0,:] = np.mean(X_train[Y_train==0,:], axis=0)\n",
    "mean_classes[1,:] = np.mean(X_train[Y_train==1,:], axis=0)\n",
    "# compute the overall mean\n",
    "mean_overall = np.mean(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the within class scatter matrix\n",
    "S_W = np.zeros((no_of_PCs,no_of_PCs))\n",
    "for i in range(2):\n",
    "    S_i = np.zeros((no_of_PCs,no_of_PCs))\n",
    "    for j in range(X_train[Y_train==i,:].shape[0]):\n",
    "        x = X_train[Y_train==i,:][j,:].reshape(no_of_PCs,1)\n",
    "        mean = mean_classes[i,:].reshape(no_of_PCs,1)\n",
    "        S_i += (x-mean).dot((x-mean).T)\n",
    "    S_W += S_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the between class scatter matrix\n",
    "S_B = np.zeros((no_of_PCs,no_of_PCs))\n",
    "for i in range(2):\n",
    "    n_i = X_train[Y_train==i,:].shape[0]\n",
    "    mean_i = mean_classes[i,:].reshape(no_of_PCs,1)\n",
    "    mean_overall = mean_overall.reshape(no_of_PCs,1)\n",
    "    S_B += n_i*(mean_i-mean_overall).dot((mean_i-mean_overall).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the eigenvalues and eigenvectors of inv(S_W).dot(S_B)\n",
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "# sort the eigenvalues in descending order\n",
    "indices = np.argsort(eig_vals)[::-1]\n",
    "eig_vals = eig_vals[indices]\n",
    "eig_vecs = eig_vecs[:,indices]\n",
    "# select the eigenvectors corresponding to the largest eigenvalues\n",
    "eigen_vector = eig_vecs[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the data onto the new subspace\n",
    "X_train_fda = X_train.dot(eigen_vector)\n",
    "X_test_fda = X_test.dot(eigen_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the logistic regression function\n",
    "learning_rate = 0.01\n",
    "no_of_iterations = 2000\n",
    "# add a column of ones to the data\n",
    "X_train_fda = np.hstack((np.ones((X_train_fda.shape[0],1)), X_train_fda))\n",
    "# define a vector of weights of size dimension of data\n",
    "weights = np.zeros((X_train_fda.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply the weights with the data\n",
    "def combine(X, weights):\n",
    "    return X.dot(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the gradient descent algorithm to find the required weights\n",
    "prev_weights = np.zeros((X_train_fda.shape[1],1))\n",
    "for i in range(no_of_iterations):\n",
    "    z_value = combine(X_train_fda, weights)\n",
    "    Y_predicted = sigmoid(z_value)\n",
    "    # round the values to 0 or 1\n",
    "    Y_predicted[Y_predicted>=0.5] = 1\n",
    "    Y_predicted[Y_predicted<0.5] = 0\n",
    "    # compute the error\n",
    "    error = Y_predicted - Y_train.reshape(Y_train.shape[0],1)\n",
    "    # compute the gradient\n",
    "    gradient = X_train_fda.T.dot(error)\n",
    "    # assign the current weights to the previous weights\n",
    "    prev_weights = weights\n",
    "    # update the weights\n",
    "    weights = weights - learning_rate*gradient/X_train_fda.shape[0]\n",
    "    # check if the weights have converged \n",
    "    if np.linalg.norm(weights-prev_weights) < 1e-4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  91.66666666666666 %\n"
     ]
    }
   ],
   "source": [
    "# compute the test accuracy\n",
    "X_test_fda = np.hstack((np.ones((X_test_fda.shape[0],1)), X_test_fda))\n",
    "z_value = combine(X_test_fda, weights)\n",
    "Y_predicted = sigmoid(z_value)\n",
    "Y_predicted[Y_predicted>=0.5] = 1\n",
    "Y_predicted[Y_predicted<0.5] = 0\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for i in range(Y_test.shape[0]):\n",
    "    if Y_test[i] == 0 and Y_predicted[i] == 0:\n",
    "        count_0 += 1\n",
    "    elif Y_test[i] == 1 and Y_predicted[i] == 1:\n",
    "        count_1 += 1\n",
    "print('Test accuracy: ', (count_0+count_1)/Y_test.shape[0]*100 ,'%')\n",
    "test_accuracy_pca = (count_0+count_1)/Y_test.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples:  60\n",
      "Number of training samples:  237\n",
      "Test accuracy with PCA + LDA:  91.66666666666666 %\n",
      "Test accuracy with LDA:  85.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Number of test samples: ', Y_test.shape[0])\n",
    "print('Number of training samples: ', Y_train.shape[0])\n",
    "print('Test accuracy with PCA + LDA: ', test_accuracy_pca, '%')\n",
    "print('Test accuracy with LDA: ', test_accuracy_lda, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
