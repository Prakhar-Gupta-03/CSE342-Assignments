{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for a node in the decision tree\n",
    "class Node:\n",
    "    # constructor for the node class\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None, parent = None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        self.parent = parent\n",
    "    # check if node is a leaf node\n",
    "    def check_if_leaf_leaf(self):\n",
    "        if (self.left == None and self.right == None):\n",
    "            return True\n",
    "        return False\n",
    "    # check if node is a root node\n",
    "    def check_if_root(self):\n",
    "        if (self.parent == None):\n",
    "            return True\n",
    "        return False\n",
    "    # check if node is a left child\n",
    "    def check_if_left_child(self):\n",
    "        if (self.parent.left == self):\n",
    "            return True\n",
    "        return False\n",
    "    # check if node is a right child\n",
    "    def check_if_right_child(self):\n",
    "        if (self.parent.right == self):\n",
    "            return True\n",
    "        return False\n",
    "    # check if node is a parent\n",
    "    def check_if_parent(self):\n",
    "        if (self.left != None or self.right != None):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the decision tree class for classification\n",
    "class DecisionTree:\n",
    "    # defining the constructor\n",
    "    def __init__(self, no_of_minimum_sample_splits = 2, maximum_depth = 1000):\n",
    "        self.no_of_minimum_sample_splits = no_of_minimum_sample_splits\n",
    "        self.maximum_depth = maximum_depth\n",
    "    # defining the function to compute the gini index\n",
    "    def compute_gini_index(self, y):\n",
    "        classes = np.unique(y, return_counts=True)\n",
    "        impurity = 1\n",
    "        for i in classes[1]:\n",
    "            impurity -= (i/len(y))**2\n",
    "        return impurity\n",
    "    # defining the function to compute the information gain\n",
    "    def compute_information_gain(self, y, y1, y2):\n",
    "        # information gain = entropy(parent) - [weighted average]entropy(children)\n",
    "        left_weight = len(y1)/len(y)\n",
    "        right_weight = len(y2)/len(y)\n",
    "        information_gain = self.compute_gini_index(y) - (left_weight*self.compute_gini_index(y1) + right_weight*self.compute_gini_index(y2))\n",
    "        return information_gain\n",
    "    # defining the recursive function to build the decision tree\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        sample_num = X.shape[0]\n",
    "        feature_num = X.shape[1]\n",
    "        class_num = len(np.unique(y))\n",
    "        # if the number of samples is greater than the minimum sample split and the depth is less than the maximum depth, then the tree is not built further\n",
    "        if sample_num >= self.no_of_minimum_sample_splits and depth <= self.maximum_depth:\n",
    "            best_information_gain = 0\n",
    "            best_feature = None\n",
    "            best_threshold = None\n",
    "            for i in range(feature_num):\n",
    "                thresholds = np.unique(X[:, i])\n",
    "                for j in thresholds:\n",
    "                    y_left = y[X[:, i] <= j]\n",
    "                    y_right = y[X[:, i] > j]\n",
    "                    # if the left or the right size is empty, then it means that the threshold is not valid\n",
    "                    if len(y_left) == 0 or len(y_right) == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        # computing the information gain\n",
    "                        information_gain = self.compute_information_gain(y, y_left, y_right)\n",
    "                        # if the information gain is greater than the best information gain, then the best information gain is updated\n",
    "                        if information_gain > best_information_gain:\n",
    "                            best_information_gain = information_gain\n",
    "                            best_feature = i\n",
    "                            best_threshold = j\n",
    "            # if the best feature is not None, then the tree is built further\n",
    "            if best_feature != None:\n",
    "                left_indices = np.argwhere(X[:, best_feature] <= best_threshold).flatten()\n",
    "                right_indices = np.argwhere(X[:, best_feature] > best_threshold).flatten()\n",
    "                left = self.build_tree(X[left_indices, :], y[left_indices], depth+1)\n",
    "                right = self.build_tree(X[right_indices, :], y[right_indices], depth+1)\n",
    "                return Node(best_feature, best_threshold, left, right)\n",
    "        # if the number of samples is less than the minimum sample split or the depth is greater than the maximum depth, then the leaf node is returned\n",
    "        leaf_value = np.argmax(np.bincount(y))\n",
    "        return Node(value=leaf_value)\n",
    "    # defining the function to fit the decision tree\n",
    "    def train(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "    # defining the function to predict the output\n",
    "    def predict_output(self, x, node):\n",
    "        if node.check_if_leaf_leaf():\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.predict_output(x, node.left)\n",
    "        return self.predict_output(x, node.right)\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            y_pred.append(self.predict_output(x, self.root))\n",
    "        return np.array(y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# defining the decision tree \n",
    "decision_tree = DecisionTree()\n",
    "# fitting the decision tree\n",
    "decision_tree.train(X_train, y_train)\n",
    "# predicting the output\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "# printing the accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Feature: petal length (cm) Threshold: 1.7\n",
      "\t2 Leaf Node: 0 Class: setosa\n",
      "\t3 Feature: petal width (cm) Threshold: 1.7\n",
      "\t\t4 Feature: petal length (cm) Threshold: 4.9\n",
      "\t\t\t5 Feature: petal width (cm) Threshold: 1.6\n",
      "\t\t\t\t6 Leaf Node: 1 Class: versicolor\n",
      "\t\t\t\t7 Leaf Node: 2 Class: virginica\n",
      "\t\t\t8 Feature: petal width (cm) Threshold: 1.5\n",
      "\t\t\t\t9 Leaf Node: 2 Class: virginica\n",
      "\t\t\t\t10 Feature: sepal length (cm) Threshold: 6.7\n",
      "\t\t\t\t\t11 Leaf Node: 1 Class: versicolor\n",
      "\t\t\t\t\t12 Leaf Node: 2 Class: virginica\n",
      "\t\t13 Feature: petal length (cm) Threshold: 4.8\n",
      "\t\t\t14 Feature: sepal length (cm) Threshold: 5.9\n",
      "\t\t\t\t15 Leaf Node: 1 Class: versicolor\n",
      "\t\t\t\t16 Leaf Node: 2 Class: virginica\n",
      "\t\t\t17 Leaf Node: 2 Class: virginica\n"
     ]
    }
   ],
   "source": [
    "# printing the decision tree\n",
    "num = 1\n",
    "def print_decision_tree(node, depth=0):\n",
    "    global num\n",
    "    if node.check_if_leaf_leaf():\n",
    "        global num\n",
    "        print(depth*\"\\t\"+str(num)+\" Leaf Node:\", node.value, \"Class:\", iris.target_names[node.value])\n",
    "        num = num+1\n",
    "        return\n",
    "    print(depth*\"\\t\"+str(num)+\" Feature:\", iris.feature_names[node.feature], \"Threshold:\", node.threshold)\n",
    "    num = num+1\n",
    "    print_decision_tree(node.left, depth+1)\n",
    "    print_decision_tree(node.right, depth+1)\n",
    "print_decision_tree(decision_tree.root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
